{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["e8deba664d755363","gNEEvUazVMr5","TZmOZNCYU4Oz","difNKQa1Kewd","1eecb40a1254a785","9cf7ffc9f937344e"],"toc_visible":true,"machine_shape":"hm","gpuType":"V100","authorship_tag":"ABX9TyOMBlJ+DOVByDFNA6n7UoyH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Installations & Imports"],"metadata":{"collapsed":false,"id":"e8deba664d755363"}},{"cell_type":"code","source":["!pip install deap\n","!pip install gymnasium\n","!pip install ray[rllib]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"khP-pkMYRhbE","executionInfo":{"status":"ok","timestamp":1710893329545,"user_tz":-120,"elapsed":24698,"user":{"displayName":"Achilleas Palaskos","userId":"04608072963754583562"}},"outputId":"e518036f-6614-4e25-8baa-29be23ae7e11"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting deap\n","  Downloading deap-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (135 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/135.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deap) (1.25.2)\n","Installing collected packages: deap\n","Successfully installed deap-1.4.1\n","Collecting gymnasium\n","  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.10.0)\n","Collecting farama-notifications>=0.0.1 (from gymnasium)\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","Installing collected packages: farama-notifications, gymnasium\n","Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n","Collecting ray[rllib]\n","  Downloading ray-2.9.3-cp310-cp310-manylinux2014_x86_64.whl (64.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (8.1.7)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (3.13.1)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (4.19.2)\n","Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (1.0.8)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (24.0)\n","Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (3.20.3)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (6.0.1)\n","Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (1.3.1)\n","Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (1.4.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (2.31.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (1.5.3)\n","Collecting tensorboardX>=1.9 (from ray[rllib])\n","  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (14.0.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (2023.6.0)\n","Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (0.1.8)\n","Collecting gymnasium==0.28.1 (from ray[rllib])\n","  Downloading gymnasium-0.28.1-py3-none-any.whl (925 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m925.5/925.5 kB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting lz4 (from ray[rllib])\n","  Downloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (0.19.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (1.11.4)\n","Requirement already satisfied: typer in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (0.9.0)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (13.7.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1->ray[rllib]) (1.25.2)\n","Collecting jax-jumpy>=1.0.0 (from gymnasium==0.28.1->ray[rllib])\n","  Downloading jax_jumpy-1.0.0-py3-none-any.whl (20 kB)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1->ray[rllib]) (2.2.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1->ray[rllib]) (4.10.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1->ray[rllib]) (0.0.4)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[rllib]) (23.2.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[rllib]) (2023.12.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[rllib]) (0.33.0)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[rllib]) (0.18.0)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[rllib]) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[rllib]) (2023.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray[rllib]) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray[rllib]) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray[rllib]) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray[rllib]) (2024.2.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->ray[rllib]) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->ray[rllib]) (2.16.1)\n","Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ray[rllib]) (3.2.1)\n","Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ray[rllib]) (9.4.0)\n","Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ray[rllib]) (2.31.6)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ray[rllib]) (2024.2.12)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ray[rllib]) (1.5.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->ray[rllib]) (0.1.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->ray[rllib]) (1.16.0)\n","Installing collected packages: tensorboardX, lz4, jax-jumpy, gymnasium, ray\n","  Attempting uninstall: gymnasium\n","    Found existing installation: gymnasium 0.29.1\n","    Uninstalling gymnasium-0.29.1:\n","      Successfully uninstalled gymnasium-0.29.1\n","Successfully installed gymnasium-0.28.1 jax-jumpy-1.0.0 lz4-4.3.3 ray-2.9.3 tensorboardX-2.6.2.2\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"is_executing":true,"id":"initial_id","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710893339790,"user_tz":-120,"elapsed":10248,"user":{"displayName":"Achilleas Palaskos","userId":"04608072963754583562"}},"outputId":"a3dcca7d-445c-493f-849d-c6bd2aa6ebd8"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n","  if (distutils.version.LooseVersion(tf.__version__) <\n"]}],"source":["import os\n","import math\n","import random\n","import gymnasium as gym\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import seaborn as sns\n","import torch\n","import torch.nn as nn\n","from typing import Any, Callable, Dict, List, Optional, Tuple\n","from deap import base, creator, tools, algorithms\n","import ray\n","from ray.rllib.algorithms.algorithm import Algorithm\n","from ray.rllib.algorithms.ppo import PPOConfig\n","from tqdm import tqdm\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","source":["# Classes & Helpers"],"metadata":{"id":"Ju5El-2RBCcL"}},{"cell_type":"markdown","metadata":{"id":"gNEEvUazVMr5"},"source":["## Data"]},{"cell_type":"code","execution_count":70,"metadata":{"id":"TgN6P2ZxVOJS","executionInfo":{"status":"ok","timestamp":1710895881802,"user_tz":-120,"elapsed":434,"user":{"displayName":"Achilleas Palaskos","userId":"04608072963754583562"}}},"outputs":[],"source":["class Data():\n","    def __init__(self,\n","                 x_min, x_max,\n","                 t_min, t_max,\n","                 test_dir,\n","                 eps=1e-5,\n","                 device='cpu',\n","                 dtype=torch.float32):\n","\n","        self.x_min = x_min\n","        self.x_max = x_max\n","        self.t_min = t_min\n","        self.t_max = t_max\n","        self.test_dir = test_dir\n","        self.eps = eps\n","        self.device = device\n","        self.dtype = dtype\n","\n","\n","    def _generate_random_numbers(self, min_, max_, N):\n","        return min_ + (max_ - min_) * torch.rand(size=(N,), dtype=self.dtype)\n","\n","\n","    # *** Create in-domain points ***\n","    def sample_domain(self, N_domain, x_min, x_max, t_min, t_max):\n","        # Random Grid\n","        x_domain = self._generate_random_numbers(x_min, x_max, N_domain)\n","        t_domain = self._generate_random_numbers(t_min, t_max, N_domain)\n","        domain_data = torch.stack((x_domain, t_domain), dim=1)\n","        return torch.tensor(domain_data, dtype=self.dtype, device=self.device, requires_grad=True)\n","\n","\n","    # *** Boundary Conditions ***\n","    def sample_boundary(self, Nt_bound):\n","        # Random boundary points\n","        t_bound = self._generate_random_numbers(self.t_min, self.t_max, Nt_bound)\n","        x_left = - torch.ones(1, dtype=self.dtype)\n","        x_right = torch.ones(1, dtype=self.dtype)\n","\n","        bound_data_left = torch.stack(torch.meshgrid(x_left, t_bound)).view(2, -1).permute(1, 0)\n","        bound_data_right = torch.stack(torch.meshgrid(x_right, t_bound)).view(2, -1).permute(1, 0)\n","        bound_data = torch.cat([bound_data_left, bound_data_right]).requires_grad_(True).to(self.device)\n","\n","        u_bound = torch.zeros(len(bound_data), 1, dtype=self.dtype, device=self.device)\n","\n","        return bound_data, u_bound\n","\n","\n","    # *** Initial Condition ***\n","    def sample_initial(self, Nx_init, x_min, x_max):\n","        # Random initial points\n","        x_init = self._generate_random_numbers(x_min, x_max, Nx_init)\n","        t_init = torch.zeros(1, dtype=self.dtype)\n","        init_data = torch.stack(torch.meshgrid(x_init, t_init)).view(2, -1).permute(1, 0).requires_grad_(True).to(self.device)\n","\n","        u_init = - torch.sin(math.pi * x_init)\n","\n","        return init_data, u_init\n","\n","    # *** Test set ***\n","    def sample_test(self):\n","        test_data = pd.read_csv(self.test_dir).to_numpy()\n","        return torch.tensor(test_data, dtype=self.dtype, device=self.device, requires_grad=True)"]},{"cell_type":"markdown","metadata":{"id":"TZmOZNCYU4Oz"},"source":["## Network"]},{"cell_type":"code","source":["class MLP(nn.Module):\n","    def __init__(self, layers, activation=nn.Tanh(), weight_init=None, bias_init=None, device='cpu'):\n","        super().__init__()\n","        self.n_layers = len(layers) - 1\n","        self.layers = layers\n","        self.activation = activation\n","        self.weight_init = weight_init\n","        self.bias_init = bias_init\n","\n","        dense_layers = [\n","            self.dense_layer(in_features=self.layers[i], out_features=self.layers[i + 1])\n","            for i in range(self.n_layers - 1)]\n","        dense_layers.append(nn.Linear(in_features=self.layers[-2], out_features=self.layers[-1]))\n","\n","        self.mlp = nn.Sequential(*dense_layers).to(device)\n","\n","    def dense_layer(self, in_features, out_features):\n","        dense_layer = nn.Sequential(\n","            nn.Linear(in_features=in_features, out_features=out_features),\n","        )\n","\n","        if self.weight_init is not None:\n","            self.weight_init(dense_layer[0].weight)\n","\n","        if self.bias_init is not None:\n","            self.bias_init(dense_layer[0].bias)\n","\n","        dense_layer.add_module(\"activation\", self.activation)\n","        return dense_layer\n","\n","    def forward(self, x):\n","        return self.mlp(x)"],"metadata":{"id":"BrYO4dPrrWP9","executionInfo":{"status":"ok","timestamp":1710895882400,"user_tz":-120,"elapsed":9,"user":{"displayName":"Achilleas Palaskos","userId":"04608072963754583562"}}},"execution_count":71,"outputs":[]},{"cell_type":"markdown","source":["## PINN-Base"],"metadata":{"id":"difNKQa1Kewd"}},{"cell_type":"code","source":["class PINNBase():\n","    def __init__(self,\n","                 layers,\n","                 activation,\n","                 device):\n","\n","        self.v = 0.01 / math.pi\n","\n","        # Define the model\n","        self.model = MLP(layers=layers,\n","                         activation=activation,\n","                         weight_init=lambda m: nn.init.xavier_normal_(m.data, nn.init.calculate_gain('tanh')),\n","                         bias_init=lambda m: nn.init.zeros_(m.data),\n","                         device=device)\n","\n","        # Set the optimizers\n","        adam = torch.optim.Adam(self.model.parameters())\n","        lbfgs = torch.optim.LBFGS(self.model.parameters(),\n","                                  lr=1,\n","                                  max_iter=5,\n","                                #   max_iter=5000,\n","                                  max_eval=None,\n","                                  tolerance_grad=1e-07,\n","                                  tolerance_change=1e-09,\n","                                  history_size=100,\n","                                  line_search_fn='strong_wolfe')\n","\n","        self.optimizers = {\"adam\": adam, \"lbfgs\": lbfgs}\n","\n","        # Set the Loss function\n","        self.criterion = nn.MSELoss()\n","\n","        # Set the MAE criterion for test data only\n","        self.l1_loss = nn.L1Loss()\n","\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","\n","    def grad(self, output, input):\n","        return torch.autograd.grad(\n","                    output, input,\n","                    grad_outputs=torch.ones_like(output),\n","                    retain_graph=True,\n","                    create_graph=True\n","                )[0]\n","\n","\n","    def calculate_pde_residual(self, x):\n","        # Forward pass\n","        u = self.forward(x)\n","\n","        # Calculate 1st and 2nd derivatives\n","        du_dX = self.grad(u, x)\n","        du_dXX = self.grad(du_dX, x)\n","\n","        # Retrieve the partial gradients\n","        du_dt = du_dX[:, 1].flatten()\n","        du_dx = du_dX[:, 0].flatten()\n","        du_dxx = du_dXX[:, 0].flatten()\n","\n","        pde_res = du_dt + u.flatten() * du_dx - self.v * du_dxx\n","\n","        return u, pde_res\n","\n","\n","    def calculate_pde_loss(self, data):\n","        # Calculate the domain loss\n","        _, self.pde_res = self.calculate_pde_residual(data)\n","        pde_target = torch.zeros_like(self.pde_res)\n","        return self.criterion(self.pde_res, pde_target)\n","\n","\n","    def calculate_total_loss(self, data):\n","        # Calculate boundary loss\n","        loss_b = self.criterion(\n","            self.forward(data[\"bound_data\"]).flatten(),\n","            data[\"u_bound\"].flatten()\n","        )\n","\n","        # Calculate initial loss\n","        loss_i = self.criterion(\n","            self.forward(data[\"init_data\"]).flatten(),\n","            data[\"u_init\"].flatten()\n","        )\n","\n","        # Calculate the domain loss\n","        domain_data = torch.cat((data[\"domain_data\"], data[\"anchors\"]), dim=0)\n","        loss_pde = self.calculate_pde_loss(domain_data)\n","\n","        # Calculate total discriminator loss\n","        return loss_b + loss_i + loss_pde\n","\n","\n","    def evaluate_pinn(self, test_data):\n","        _, pde_res = self.calculate_pde_residual(test_data)\n","        pde_target = torch.zeros_like(pde_res)\n","        return self.l1_loss(pde_res, pde_target)\n","\n","\n","    def train_step(self, data):\n","        loss = self.calculate_total_loss(data)\n","        loss.backward()\n","        return loss\n","\n","\n","    def closure(self):\n","        self.lbfgs_optimizer.zero_grad()\n","        return self.train_step(self.data)"],"metadata":{"id":"GjJ_f5zo63WW","executionInfo":{"status":"ok","timestamp":1710897081494,"user_tz":-120,"elapsed":500,"user":{"displayName":"Achilleas Palaskos","userId":"04608072963754583562"}}},"execution_count":95,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hNERHKrzFCeB"},"source":["## PINN"]},{"cell_type":"code","source":["class PINN():\n","    def __init__(self,\n","                 x_min, x_max,\n","                 t_min, t_max,\n","                 N_domain,\n","                 Nx_init,\n","                 Nt_bound,\n","                 test_dir,\n","                 general_max_episode_steps,\n","                 layers, activation,\n","                 checkpoint_path,\n","                 eps=1e-5,\n","                 device='cpu',\n","                 dtype=torch.float32):\n","\n","        # Constants\n","        self.checkpoint_path = checkpoint_path\n","        self.device = device\n","        self.dtype = dtype\n","        self.x_min = x_min\n","        self.x_max = x_max\n","        self.t_min = t_min\n","        self.t_max = t_max\n","        self.general_max_episode_steps = general_max_episode_steps\n","        self.data = {}\n","\n","        # Create real data\n","        self.data_init = Data(x_min, x_max,\n","                              t_min, t_max,\n","                              test_dir,\n","                              eps,\n","                              device,\n","                              dtype)\n","\n","        # Create train data\n","        self.data[\"domain_data\"] = self.data_init.sample_domain(N_domain, self.x_min, self.x_max, self.t_min, self.t_max)\n","        self.data[\"bound_data\"], self.data[\"u_bound\"] = self.data_init.sample_boundary(Nt_bound)\n","        self.data[\"init_data\"], self.data[\"u_init\"] = self.data_init.sample_initial(Nx_init, self.x_min, self.x_max)\n","\n","        # Create test data\n","        self.test_data = self.data_init.sample_test()\n","\n","        # Create base PINN\n","        self.base_pinn = PINNBase(layers, activation, device)\n","\n","\n","    def add_anchors(self, step, point):\n","        if step == 0:\n","            self.data[\"anchors\"] = torch.empty(self.general_max_episode_steps, 2)\n","        else:\n","            new_anchors = torch.tensor(point, dtype=self.dtype, device=self.device, requires_grad=True).view(-1, 2)\n","            self.data[\"anchors\"] = torch.cat((self.data[\"anchors\"], new_anchors), dim=0)\n","\n","\n","    def train_with_adam(self, N_adam, data):\n","        optimizer = self.base_pinn.optimizers['adam']\n","\n","        for epoch in range(1, N_adam + 1):\n","            optimizer.zero_grad()\n","            loss = self.base_pinn.train_step(data)\n","            optimizer.step()\n","\n","        return loss\n","\n","\n","    def train_with_lbfgs(self, N_lbfgs, data):\n","        self.base_pinn.lbfgs_optimizer = self.base_pinn.optimizers[\"lbfgs\"]\n","        self.base_pinn.data = data\n","\n","        for epoch in range(1, N_lbfgs + 1):\n","            loss = self.base_pinn.lbfgs_optimizer.step(self.base_pinn.closure)\n","\n","        return loss\n","\n","\n","    def checkpoint(self):\n","        torch.save({\n","            \"model\": self.base_pinn.model.state_dict()\n","        }, self.checkpoint_path)\n","\n","\n","    def format_loss(self, loss):\n","        if loss == 0:\n","            return \"0.0e+00\"\n","\n","        # Calculate the exponent part\n","        exponent = int(math.log10(abs(loss)))\n","\n","        # Determine the format based on the value of the loss\n","        if abs(loss) < 1:\n","            formatted_loss = f\"{loss:.2e}\"\n","        else:\n","            # Adjust the sign of the formatted loss\n","            sign = \"-\" if loss < 0 else \"\"\n","\n","            # Calculate the number of decimal places\n","            decimal_places = 2 - exponent\n","\n","            # Ensure at least two decimal places\n","            decimal_places = max(decimal_places, 2)\n","\n","            # Format the loss with the correct sign\n","            formatted_loss = f\"{sign}{abs(loss):.{decimal_places}e}\"\n","\n","        return formatted_loss\n","\n","\n","    def keep_checkpoints_and_print_losses(self, iter, patience, print_every, loss, loss_test):\n","\n","        loss_str = self.format_loss(loss)\n","        loss_test_str = self.format_loss(loss_test)\n","\n","        if iter == 1:\n","            self.best_val_loss = loss_test\n","            self.best_epoch = -1\n","            self.checkpoint()\n","            self.flag = 1\n","            print(f\"Iteration: {iter} | loss: {loss_str} | test_mae: {loss_test_str} - *Checkpoint*\")\n","        else:\n","            if loss_test < self.best_val_loss:\n","                self.best_val_loss = loss_test\n","                self.best_epoch = iter\n","                self.checkpoint()\n","                self.flag = 1\n","                if iter % print_every == 0:\n","                    print(f\"Iteration: {iter} | loss: {loss_str} | test_mae: {loss_test_str} - *Checkpoint*\")\n","            elif iter - self.best_epoch > patience:\n","                if iter % print_every == 0:\n","                    self.early_stopping_applied = 1\n","                    print(f\"Iteration: {iter} | loss: {loss_str} | test_mae: {loss_test_str}\")\n","                return\n","\n","        if (self.flag == 0) and (iter % print_every == 0):\n","            print(f\"Iteration: {iter} | loss: {loss_str} | test_mae: {loss_test_str}\")\n","\n","\n","    def train(self, iters, patience, print_every, N_adam, N_lbfgs):\n","        print(f\"PINN: {iters} iterations\")\n","        print(f\"a. PINN: {N_adam} epochs --> Adam\")\n","        print(f\"b. PINN: {N_lbfgs} epochs --> L-BFGS\")\n","\n","        for iter in tqdm(range(1, iters + 1)):\n","            self.flag = 0\n","            self.early_stopping_applied = 0\n","\n","            # Train with adam\n","            print(f\"\\nTraining with ADAM...\")\n","            loss = self.train_with_adam(N_adam, self.data)\n","\n","            # Train with L-BFGS\n","            print(f\"\\nTraining with L-BFGS...\")\n","            loss = self.train_with_lbfgs(N_lbfgs, self.data)\n","\n","            # Evaluate on test\n","            loss_test = self.base_pinn.evaluate_pinn(self.test_data)\n","\n","            # Keep check points and print losses\n","            self.keep_checkpoints_and_print_losses(iter, patience, print_every, loss, loss_test)\n","            if self.early_stopping_applied:\n","                print(f\"\\nEarly stopping applied at epoch {iter}.\")\n","                break\n","\n","        return loss_test.detach().cpu().numpy()"],"metadata":{"id":"hNUssAyw5MMh","executionInfo":{"status":"ok","timestamp":1710897504195,"user_tz":-120,"elapsed":285,"user":{"displayName":"Achilleas Palaskos","userId":"04608072963754583562"}}},"execution_count":103,"outputs":[]},{"cell_type":"markdown","source":["## Environment"],"metadata":{"collapsed":false,"id":"1eecb40a1254a785"}},{"cell_type":"code","outputs":[],"source":["class GeneralEnv(gym.Env):\n","    def __init__(self, env_config: Optional[Dict] = None):\n","        super().__init__()\n","\n","        # PINN\n","        self.pinn = env_config[\"pinn\"]\n","        self.iterations = env_config[\"iterations\"]\n","        self.patience = env_config[\"patience\"]\n","        self.print_every = env_config[\"print_every\"]\n","        self.num_epochs_adam = env_config[\"num_epochs_adam\"]\n","        self.num_epochs_lbfgs = env_config[\"num_epochs_lbfgs\"]\n","\n","        # PPO Output: One 2D Point\n","        self.action_space = gym.spaces.Box(\n","            low=np.float32([x_min, t_min]),\n","            high=np.float32([x_max, t_max]),\n","            dtype=np.float32,\n","            shape=(2,)\n","        )\n","\n","        # PPO Input: Generated 2D Point, u, pde, step\n","        self.observation_space = gym.spaces.Box(\n","            low=np.float32([x_min, t_min, -np.inf, -np.inf, 0.0]),\n","            high=np.float32([x_max, t_max, np.inf, np.inf, 1.0]),\n","            dtype=np.float32,\n","            shape=(5,)\n","        )\n","\n","        # Agent\n","        self.max_anchors = env_config['max_anchors']\n","        self.sampled_points = []\n","        self._step_counter = 0\n","        self._trial = -1\n","        self._eval_error = None\n","\n","        # Initialize GENERAL history log\n","        self._general_history_fp = 'general_error_history.csv'\n","        self._initialize_log_file()\n","\n","\n","    def _initialize_log_file(self):\n","        with open(file=self._general_history_fp, mode='w', newline='\\n') as file:\n","            s = 'trial'\n","            for i in range(self.max_anchors):\n","                s += f',x_{i + 1},t_{i + 1}'\n","            log = f'{s}\\n'\n","            file.write(log)\n","\n","\n","    def _store_trial_to_log(self):\n","        num_sampled = len(self.sampled_points)\n","        assert self._eval_error is not None and num_sampled > 0\n","        assert num_sampled == self.max_anchors\n","\n","        with open(file=self._general_history_fp, mode='a', newline='\\n') as file:\n","            s = f'{self._trial}'\n","            for point in self.sampled_points:\n","                s += f',{point[0]},{point[1]}'\n","            log = f'{s}\\n'\n","            file.write(log)\n","\n","\n","    def _construct_observation(self, action: Optional[np.ndarray]=None):\n","        # If action is None, then it randomly samples a 2D point\n","        if action is None:\n","            x_rand = np.random.uniform(low=x_min, high=x_max, size=(1, 1))\n","            t_rand = np.random.uniform(low=t_min, high=t_max, size=(1, 1))\n","            action = np.hstack((x_rand, t_rand))\n","        else:\n","            assert action.shape == (2,), f'Action should be 1D array with 2 values, got {action.shape}'\n","\n","            action = np.float32([action])\n","\n","        assert action.shape == (1, 2), f'Action should be 2D array with one 2D point, got {action.shape}'\n","\n","        action_torch = torch.tensor(action, dtype=self.pinn.dtype, device=self.pinn.device, requires_grad=True)\n","        u, pde_res = self.pinn.base_pinn.calculate_pde_residual(x=action_torch)\n","        u = u.detach().cpu().numpy()\n","        pde_res = np.expand_dims(pde_res.detach().cpu().numpy(), axis=-1)\n","        normalized_step = np.float32([[self._step_counter/self.max_anchors]])\n","\n","        assert u.shape == pde_res.shape == (1, 1), f'PINN output is expected to be (1,1) for a single 2D point, got {u.shape} and {pde_res.shape}'\n","\n","        new_obs = np.squeeze(np.hstack((action, u, pde_res, normalized_step)))\n","\n","        return np.squeeze(new_obs)\n","\n","\n","    def reset(\n","            self,\n","            *,\n","            seed: int | None = None,\n","            options: dict[str, Any] | None = None,\n","    ) -> Tuple[np.ndarray, dict[str, Any]]:\n","\n","        # Compute PINN error & Store it to history log file\n","        self._step_counter = 0\n","        self._trial += 1\n","\n","        # Initialize an empty anchors tensor\n","        self.pinn.add_anchors(step=self._step_counter, point=None)\n","        self.sampled_points = []\n","\n","        return self._construct_observation(action=None)\n","\n","\n","    def step(self, action: np.ndarray) -> tuple[np.ndarray, float, bool, bool, dict[str, Any]]:\n","        self._step_counter += 1\n","\n","        # Add sampled points to training data\n","        self.pinn.add_anchors(self._step_counter, action)\n","        self.sampled_points.append(action)\n","\n","        # Generating next input\n","        next_obs = self._construct_observation(action=action)\n","\n","        # If batch is completed Then train, reward the agent and restart episode (sampling process). Finally store log\n","        # Else continue episode (sampling process) and no reward is provided\n","        if self._step_counter < self.max_anchors:\n","            reward = 0.0\n","            done = False\n","        else:\n","            self._eval_error = self.pinn.train(self.iterations, self.patience, self.print_every, self.num_epochs_adam, self.num_epochs_lbfgs)\n","            self._store_trial_to_log()\n","            reward = float(self._eval_error)\n","            done = True\n","\n","        # Return transition tuple: next observation, reward, done, truncate=False, info=None\n","        return next_obs, reward, done, {}\n","\n","\n","    def render(self):\n","        raise NotImplementedError('Render function is not supported')"],"metadata":{"id":"48cde64199497050","executionInfo":{"status":"ok","timestamp":1710897505446,"user_tz":-120,"elapsed":555,"user":{"displayName":"Achilleas Palaskos","userId":"04608072963754583562"}}},"execution_count":104},{"cell_type":"markdown","source":["# Configuration"],"metadata":{"collapsed":false,"id":"9cf7ffc9f937344e"}},{"cell_type":"code","source":["# Data\n","x_min, x_max = -1, 1\n","t_min, t_max = 0, 1\n","N_domain = 2_500        # Number of domain training points\n","Nt_bound = 20           # Number of training points for x=-1 and x=1\n","Nx_init = 10            # Number of training points for t=0\n","test_dir = 'test_data.csv'\n","\n","# Model\n","N_layers = 3\n","N_neurons = 20\n","layers = [2] + N_layers * [N_neurons] + [1]\n","hidden_activation = nn.Tanh()\n","\n","# Other\n","dtype = torch.float32\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","\n","# Training\n","iterations = 2\n","patience = iterations\n","print_every = 1\n","num_epochs_adam = 3\n","num_epochs_lbfgs = 1"],"metadata":{"id":"FJMXOWQCBJ6R","executionInfo":{"status":"ok","timestamp":1710897518859,"user_tz":-120,"elapsed":616,"user":{"displayName":"Achilleas Palaskos","userId":"04608072963754583562"}}},"execution_count":105,"outputs":[]},{"cell_type":"code","outputs":[{"output_type":"stream","name":"stderr","text":["2024-03-20 00:29:33,637\tINFO worker.py:1724 -- Started a local Ray instance.\n"]},{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7c29acf8b190>"]},"metadata":{},"execution_count":48}],"source":["trials = 10\n","\n","# GENERAL Config\n","ray.shutdown()\n","ray.init()\n","general_batch_mode = 'complete_episodes'\n","general_episode_steps = 16\n","general_train_epochs = 10\n","general_critic = True\n","general_gae = True\n","general_lambda = 0.95\n","general_gamma = 0.99\n","general_sgd_minibatch_size = general_episode_steps\n","general_train_batch_size = general_episode_steps\n","general_shuffle_sequences = True\n","general_clip_param = 0.2\n","general_vf_loss_coeff = 0.5\n","general_learning_rate = 0.0005\n","\n","# Checkpoint Filepaths\n","fcnet_directory = 'burger/checkpoints/fcnet'\n","baseline_directory = 'burger/checkpoints/models/baseline'\n","random_resampling_direcotry = 'burger/checkpoints/models/random-resampling'\n","rar_directory = 'burger/checkpoints/models/rar'\n","ms_rar_directory = 'burger/checkpoints/models/ms-rar'\n","genesis_directory = 'burger/checkpoints/models/genesis'\n","epsilon_greedy_directory = 'burger/checkpoints/models/epsilon-greedy'\n","ganpoint_directory = 'burger/checkpoints/models/ganpoint'\n","general_directory = f'burger/checkpoints/models/general'\n","general_rl_directory = f'{general_directory}/agent'\n","\n","# Set seeds\n","seed = 0\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)"],"metadata":{"is_executing":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"89559ba18b8a9f3b","executionInfo":{"status":"ok","timestamp":1710894575145,"user_tz":-120,"elapsed":4536,"user":{"displayName":"Achilleas Palaskos","userId":"04608072963754583562"}},"outputId":"170415b6-3a69-4443-cf9e-823e322e017f"},"execution_count":null},{"cell_type":"markdown","source":["# MAIN"],"metadata":{"id":"qcQP9fRcFEt_"}},{"cell_type":"code","source":["# Initialize a PINN object based on the above configuration\n","pinn = PINN(\n","    x_min, x_max,\n","    t_min, t_max,\n","    N_domain,\n","    Nx_init,\n","    Nt_bound,\n","    test_dir,\n","    general_episode_steps,\n","    layers,\n","    hidden_activation,\n","    \"pinn_model.pth\"    # general_directory\n",")\n","\n","agent_config = PPOConfig()\n","agent_config.model.update({\n","    'use_lstm': True,\n","    'vf_share_layers': True,\n","    'max_seq_len': general_episode_steps,\n","    'lstm_cell_size': 128,\n","})\n","agent_config.rollouts(\n","    num_rollout_workers=1,\n","    batch_mode=general_batch_mode,\n","    rollout_fragment_length=general_episode_steps\n",")\n","agent_config.use_critic = general_critic\n","agent_config.use_gae = general_gae\n","agent_config.clip_param = general_clip_param\n","agent_config.sgd_minibatch_size = general_sgd_minibatch_size\n","agent_config.shuffle_sequences = general_shuffle_sequences\n","agent_config.train_batch_size = general_episode_steps\n","agent_config.vf_loss_coeff = general_vf_loss_coeff\n","agent_config.seed = seed\n","agent_config.gamma = general_gamma\n","agent_config.lr = general_learning_rate\n","agent_config.num_gpus = 1\n","agent_config.environment(disable_env_checking=True)\n","agent_config.framework('torch')\n","\n","env_config = {\n","    'pinn': pinn,\n","    'max_anchors': general_episode_steps,\n","    'iterations': iterations,\n","    'patience': patience,\n","    'print_every': print_every,\n","    'num_epochs_adam': num_epochs_adam,\n","    'num_epochs_lbfgs': num_epochs_lbfgs,\n","    'device': device,\n","}\n","\n","agent = agent_config.environment(env=GeneralEnv, env_config=env_config).build()\n","\n","for trial in range(trials):\n","    print(f\"\\n\\n******************** trial = {trial} ********************\")\n","    agent.train()"],"metadata":{"id":"ALyvXxFLFHyg","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1710897529012,"user_tz":-120,"elapsed":7698,"user":{"displayName":"Achilleas Palaskos","userId":"04608072963754583562"}},"outputId":"ae01c80f-13c5-4c49-c74a-55995b6d7bbc"},"execution_count":106,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[36m(pid=20995)\u001b[0m /usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:121: DeprecationWarning: pkg_resources is deprecated as an API\n","\u001b[36m(pid=20995)\u001b[0m   warnings.warn(\"pkg_resources is deprecated as an API\", DeprecationWarning)\n","\u001b[36m(pid=20995)\u001b[0m /usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:2870: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n","\u001b[36m(pid=20995)\u001b[0m Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n","\u001b[36m(pid=20995)\u001b[0m   declare_namespace(pkg)\n","\u001b[36m(pid=20995)\u001b[0m /usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:2870: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n","\u001b[36m(pid=20995)\u001b[0m Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n","\u001b[36m(pid=20995)\u001b[0m   declare_namespace(pkg)\n","\u001b[36m(pid=20995)\u001b[0m /usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:2870: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.\n","\u001b[36m(pid=20995)\u001b[0m Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n","\u001b[36m(pid=20995)\u001b[0m   declare_namespace(pkg)\n","\u001b[36m(pid=20995)\u001b[0m /usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:2870: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\n","\u001b[36m(pid=20995)\u001b[0m Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n","\u001b[36m(pid=20995)\u001b[0m   declare_namespace(pkg)\n","\u001b[36m(pid=20995)\u001b[0m /usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n","\u001b[36m(pid=20995)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m 2024-03-20 01:18:47,364\tWARNING utils.py:160 -- `config.auto_wrap_old_gym_envs` is activated AND you seem to have provided an old gym-API environment. RLlib will therefore try to auto-fix the following error. However, please consider switching over to the new `gymnasium` APIs:\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m Your environment ({}) does not abide to the new gymnasium-style API!\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m From Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m {}\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m Learn more about the most important changes here:\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m https://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m \n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m In order to fix this problem, do the following:\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m \n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m 1) Run `pip install gymnasium` on your command line.\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m 2) Change all your import statements in your code from\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m    `import gym` -> `import gymnasium as gym` OR\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m    `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m \n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m For your custom (single agent) gym.Env classes:\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m 3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m      EnvCompatibility` wrapper class.\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m 3.2) Alternatively to 3.1:\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m  - Change your `reset()` method to have the call signature 'def reset(self, *,\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m    seed=None, options=None)'\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m  - Return an additional info dict (empty dict should be fine) from your `reset()`\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m    method.\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m  - Return an additional `truncated` flag from your `step()` method (between `done` and\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m    `info`). This flag should indicate, whether the episode was terminated prematurely\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m    due to some time constraint or other kind of horizon setting.\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m \n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m For your custom RLlib `MultiAgentEnv` classes:\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m 4.1) Either wrap your old MultiAgentEnv via the provided\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m      `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m      MultiAgentEnvCompatibility` wrapper class.\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m 4.2) Alternatively to 4.1:\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m  - Change your `reset()` method to have the call signature\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m    'def reset(self, *, seed=None, options=None)'\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m  - Return an additional per-agent info dict (empty dict should be fine) from your\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m    `reset()` method.\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m  - Rename `dones` into `terminateds` and only set this to True, if the episode is really\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m    done (as opposed to has been terminated prematurely due to some horizon/time-limit\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m    setting).\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m  - Return an additional `truncateds` per-agent dictionary flag from your `step()`\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m    method, including the `__all__` key (100% analogous to your `dones/terminateds`\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m    per-agent dict).\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m    Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m    flag should indicate, whether the episode (for some agent or all agents) was\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m    terminated prematurely due to some time constraint or other kind of horizon setting.\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m \n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m /usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/compatibility.py:67: DeprecationWarning: \u001b[33mWARN: The `gymnasium.make(..., apply_api_compatibility=...)` parameter is deprecated and will be removed in v0.29. Instead use `gym.make('GymV21Environment-v0', env_name=...)` or `from shimmy import GymV21CompatibilityV0`\u001b[0m\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m   logger.deprecation(\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/rllib/models/catalog.py:785: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m   prep = cls(observation_space, options)\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/rllib/models/torch/recurrent_net.py:139: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m   super(LSTMWrapper, self).__init__(\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m 2024-03-20 01:18:48,135\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.recurrent_net.RecurrentNetwork` has been deprecated. This will raise an error in the future!\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/rllib/connectors/agent/obs_preproc.py:40: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m   self._preprocessor = get_preprocessor(obs_space)(\n","2024-03-20 01:18:48,210\tWARNING util.py:62 -- Install gputil for GPU system monitoring.\n","  0%|          | 0/2 [00:00<?, ?it/s]\n","2024-03-20 01:18:48,293\tERROR actor_manager.py:506 -- Ray error, taking actor 1 out of service. \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=20995, ip=172.28.0.12, actor_id=81d79b49fb28c0d2420d252c01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7978486ea8c0>)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/utils/actor_manager.py\", line 189, in apply\n","    raise e\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/utils/actor_manager.py\", line 178, in apply\n","    return func(self, *args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/execution/rollout_ops.py\", line 84, in <lambda>\n","    lambda w: w.sample(), local_worker=False, healthy_only=True\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 694, in sample\n","    batches = [self.input_reader.next()]\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/evaluation/sampler.py\", line 91, in next\n","    batches = [self.get_data()]\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/evaluation/sampler.py\", line 276, in get_data\n","    item = next(self._env_runner)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/evaluation/env_runner_v2.py\", line 344, in run\n","    outputs = self.step()\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/evaluation/env_runner_v2.py\", line 400, in step\n","    self._base_env.send_actions(actions_to_send)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/env/vector_env.py\", line 464, in send_actions\n","    ) = self.vector_env.vector_step(action_vector)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/env/vector_env.py\", line 360, in vector_step\n","    raise e\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/env/vector_env.py\", line 353, in vector_step\n","    results = self.envs[i].step(actions[i])\n","  File \"/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/compatibility.py\", line 111, in step\n","    obs, reward, done, info = self.env.step(action)\n","  File \"<ipython-input-104-4417b20f4c5a>\", line 123, in step\n","  File \"<ipython-input-103-77a156cb2eaf>\", line 152, in train\n","  File \"<ipython-input-103-77a156cb2eaf>\", line 71, in train_with_lbfgs\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\", line 385, in wrapper\n","    out = func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n","    return func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/optim/lbfgs.py\", line 309, in step\n","    state = self.state[self._params[0]]\n","AttributeError: 'LBFGS' object has no attribute '_params'\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","******************** trial = 0 ********************\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m PINN: 2 iterations\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m a. PINN: 3 epochs --> Adam\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m b. PINN: 1 epochs --> L-BFGS\n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m \n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m Training with ADAM...\n"]},{"output_type":"error","ename":"RayTaskError(AttributeError)","evalue":"\u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=20995, ip=172.28.0.12, actor_id=81d79b49fb28c0d2420d252c01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7978486ea8c0>)\n  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/utils/actor_manager.py\", line 189, in apply\n    raise e\n  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/utils/actor_manager.py\", line 178, in apply\n    return func(self, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/execution/rollout_ops.py\", line 84, in <lambda>\n    lambda w: w.sample(), local_worker=False, healthy_only=True\n  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 694, in sample\n    batches = [self.input_reader.next()]\n  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/evaluation/sampler.py\", line 91, in next\n    batches = [self.get_data()]\n  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/evaluation/sampler.py\", line 276, in get_data\n    item = next(self._env_runner)\n  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/evaluation/env_runner_v2.py\", line 344, in run\n    outputs = self.step()\n  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/evaluation/env_runner_v2.py\", line 400, in step\n    self._base_env.send_actions(actions_to_send)\n  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/env/vector_env.py\", line 464, in send_actions\n    ) = self.vector_env.vector_step(action_vector)\n  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/env/vector_env.py\", line 360, in vector_step\n    raise e\n  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/env/vector_env.py\", line 353, in vector_step\n    results = self.envs[i].step(actions[i])\n  File \"/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/compatibility.py\", line 111, in step\n    obs, reward, done, info = self.env.step(action)\n  File \"<ipython-input-104-4417b20f4c5a>\", line 123, in step\n  File \"<ipython-input-103-77a156cb2eaf>\", line 152, in train\n  File \"<ipython-input-103-77a156cb2eaf>\", line 71, in train_with_lbfgs\n  File \"/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/optim/lbfgs.py\", line 309, in step\n    state = self.state[self._params[0]]\nAttributeError: 'LBFGS' object has no attribute '_params'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRayTaskError(AttributeError)\u001b[0m              Traceback (most recent call last)","\u001b[0;32m<ipython-input-106-a53d529160a8>\u001b[0m in \u001b[0;36m<cell line: 54>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrial\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n\\n******************** trial = {trial} ********************\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[0mskipped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskip_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mskipped\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexception_cause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskipped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"step() needs to return a dict.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[0mskipped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskip_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/algorithm.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    850\u001b[0m         \u001b[0;31m#   evaluate after the training iteration is entirely done.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m             \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_one_training_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;31m# Sequential: Train (already done above), then evaluate.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/algorithm.py\u001b[0m in \u001b[0;36m_run_one_training_iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3040\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTRAINING_ITERATION_TIMER\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3041\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_disable_execution_plan_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3042\u001b[0;31m                         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3043\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3044\u001b[0m                         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_exec_impl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/ppo/ppo.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    405\u001b[0m                     )\n\u001b[1;32m    406\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m                     train_batch = synchronous_parallel_sample(\n\u001b[0m\u001b[1;32m    408\u001b[0m                         \u001b[0mworker_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m                         \u001b[0mmax_env_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/rllib/execution/rollout_ops.py\u001b[0m in \u001b[0;36msynchronous_parallel_sample\u001b[0;34m(worker_set, max_agent_steps, max_env_steps, concat)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;31m# Loop over remote workers' `sample()` method in parallel.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             sample_batches = worker_set.foreach_worker(\n\u001b[0m\u001b[1;32m     84\u001b[0m                 \u001b[0;32mlambda\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhealthy_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/rllib/evaluation/worker_set.py\u001b[0m in \u001b[0;36mforeach_worker\u001b[0;34m(self, func, local_worker, healthy_only, remote_worker_ids, timeout_seconds, return_obj_refs, mark_healthy)\u001b[0m\n\u001b[1;32m    703\u001b[0m         )\n\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m         \u001b[0mhandle_remote_call_result_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremote_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ignore_worker_failures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m         \u001b[0;31m# With application errors handled, return good results.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/rllib/evaluation/worker_set.py\u001b[0m in \u001b[0;36mhandle_remote_call_result_errors\u001b[0;34m(results, ignore_worker_failures)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRayTaskError(AttributeError)\u001b[0m: \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=20995, ip=172.28.0.12, actor_id=81d79b49fb28c0d2420d252c01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7978486ea8c0>)\n  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/utils/actor_manager.py\", line 189, in apply\n    raise e\n  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/utils/actor_manager.py\", line 178, in apply\n    return func(self, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/execution/rollout_ops.py\", line 84, in <lambda>\n    lambda w: w.sample(), local_worker=False, healthy_only=True\n  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 694, in sample\n    batches = [self.input_reader.next()]\n  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/evaluation/sampler.py\", line 91, in next\n    batches = [self.get_data()]\n  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/evaluation/sampler.py\", line 276, in get_data\n    item = next(self._env_runner)\n  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/evaluation/env_runner_v2.py\", line 344, in run\n    outputs = self.step()\n  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/evaluation/env_runner_v2.py\", line 400, in step\n    self._base_env.send_actions(actions_to_send)\n  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/env/vector_env.py\", line 464, in send_actions\n    ) = self.vector_env.vector_step(action_vector)\n  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/env/vector_env.py\", line 360, in vector_step\n    raise e\n  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/env/vector_env.py\", line 353, in vector_step\n    results = self.envs[i].step(actions[i])\n  File \"/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/compatibility.py\", line 111, in step\n    obs, reward, done, info = self.env.step(action)\n  File \"<ipython-input-104-4417b20f4c5a>\", line 123, in step\n  File \"<ipython-input-103-77a156cb2eaf>\", line 152, in train\n  File \"<ipython-input-103-77a156cb2eaf>\", line 71, in train_with_lbfgs\n  File \"/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/optim/lbfgs.py\", line 309, in step\n    state = self.state[self._params[0]]\nAttributeError: 'LBFGS' object has no attribute '_params'"]},{"output_type":"stream","name":"stdout","text":["\u001b[36m(RolloutWorker pid=20995)\u001b[0m \n","\u001b[36m(RolloutWorker pid=20995)\u001b[0m Training with L-BFGS...\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[36m(RolloutWorker pid=20995)\u001b[0m \r  0%|          | 0/2 [00:00<?, ?it/s]\n"]}]}]}